{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain is a framework using which we can develop applications with the help of language models.\n",
        "\n",
        "## These applications can be created using various functionalities such as buffer memory, agents, chains which are provided by Langchain.\n",
        "\n",
        "## There are several frameworks similar to Langchain such as LlamaIndex,  HayStack etc.\n",
        "\n",
        "## AGENDA\n",
        "\n",
        "*   Installation\n",
        "*   Load LLM with Langchain\n",
        "*   Prompt Template\n",
        "*   Chains\n",
        "*   Agents and Tools\n",
        "*   Buffer Memory\n",
        "*   Document Loader\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8LHj9UrVQouV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M1NduDCLL6L",
        "outputId": "8690d838-064b-4d00-9c38-22d1579eda1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.28 (from langchain)\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.26-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.31->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langsmith-0.1.26 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# 1. installing langchain\n",
        "\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. generate api keys and adding them as secrets\n",
        "\n",
        "# loading openai and huggingface api key\n",
        "\n",
        "# hf\n",
        "from google.colab import userdata\n",
        "HUGGINGFACEHUB_API_TOKEN = userdata.get('huggingface_hub')\n",
        "\n",
        "# openai\n",
        "# from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('openai_api')"
      ],
      "metadata": {
        "id": "0oAUkO-QUdrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the env variables\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n"
      ],
      "metadata": {
        "id": "dk3JQ-NAUdoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49FohesRd2fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## USING OPEN AI MODEL"
      ],
      "metadata": {
        "id": "BBP5ibP-tNni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing open ai\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5di75s1Udlc",
        "outputId": "32be690a-a4ad-4c0c-c961-b73d2c6ba3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.0-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing openai from langchain\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# temperature parameter controls the creativity of the llm.x\n",
        "# higher the temperature, more the creativity.\n",
        "# temperature ranges between 0 and 1.\n",
        "# 0 means, the response generated by the model is very safe\n",
        "# 1 means, the model can give an unrelated response in certain circumstances.\n",
        "\n",
        "llm = OpenAI(temperature = 0.9)"
      ],
      "metadata": {
        "id": "YJtvuEhOUdg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1"
      ],
      "metadata": {
        "id": "aHXpzzASr_ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "text = \"Suggest good names for a company manufacturing cricket bats\""
      ],
      "metadata": {
        "id": "_jaDdEE9Udc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting using llm\n",
        "# using the predict function - deprecated\n",
        "\n",
        "print(llm.predict(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoigOb5pUdVs",
        "outputId": "95d809ce-e3ad-415c-fab1-8938504c33ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. \"Blade Master\"\n",
            "2. \"Stingray Cricket Co.\"\n",
            "3. \"The Big Hit\"\n",
            "4. \"Thunderstrike Bats\"\n",
            "5. \"Crafted Willow\"\n",
            "6. \"Power Play Bats\"\n",
            "7. \"Precision Batsmiths\"\n",
            "8. \"The Perfect Drive Co.\"\n",
            "9. \"Swing Supreme\"\n",
            "10. \"Boundary Breakers\"\n",
            "11. \"Ace Bats Company\"\n",
            "12. \"Rapid Strike Bats\"\n",
            "13. \"The Mighty Willow\"\n",
            "14. \"Top Flight Cricket Co.\"\n",
            "15. \"Masterpiece Bats\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the invoke function to predict\n",
        "\n",
        "print(llm.invoke(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xoda8u3CUdSL",
        "outputId": "345d1875-24d0-44a0-c837-3995f97ac091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. StrikeMaster \n",
            "2. BatCrafters \n",
            "3. WillowWorks \n",
            "4. PowerBats \n",
            "5. ProSlammers \n",
            "6. The Bat Factory \n",
            "7. Cricket Craze \n",
            "8. Boundary Breakers \n",
            "9. HitMakers \n",
            "10. The Bat Brigade \n",
            "11. SpeedDemons Cricket \n",
            "12. Smashers Inc. \n",
            "13. Topspin Bats \n",
            "14. Crafted Strokes \n",
            "15. The Cricket Company \n",
            "16. Boundary Bashers \n",
            "17. Grand Slam Bats \n",
            "18. Wicket Wonders \n",
            "19. Boundary Blasters \n",
            "20. The Cricket Bat Co.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another way to predict\n",
        "\n",
        "# print(llm(text))"
      ],
      "metadata": {
        "id": "VdY13PkjUdPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2"
      ],
      "metadata": {
        "id": "LQWu8XvmsDit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.25)\n",
        "\n",
        "text = \"Suggest a few fancy names for a restaurant serving Indian cuisine\""
      ],
      "metadata": {
        "id": "XX7aQ7fNUdK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMTge528Uc_s",
        "outputId": "779b08f2-255b-48cb-94bb-655bdd3bc37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Maharaja's Palace\n",
            "2. Spice Kingdom\n",
            "3. Tandoori Nights\n",
            "4. Saffron Delight\n",
            "5. Curry House\n",
            "6. Masala Mansion\n",
            "7. Royal Indian Bistro\n",
            "8. Naan & Beyond\n",
            "9. The Taj Tasting Room\n",
            "10. Chai & Chaat Co.\n",
            "11. Bollywood Bites\n",
            "12. Namaste Kitchen\n",
            "13. The Great Indian Feast\n",
            "14. Flavors of India\n",
            "15. The Curry Club\n",
            "16. Rang Mahal\n",
            "17. The Tandoor Table\n",
            "18. Indulge in India\n",
            "19. The Masala Room\n",
            "20. The Royal Thali.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## USING HUGGINGFACE MODEL"
      ],
      "metadata": {
        "id": "BeJZa8nFtRER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiULUvAcsGzk",
        "outputId": "77f2dc9d-2c36-47ed-e420-dba40e3d4f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples"
      ],
      "metadata": {
        "id": "YbuYaGb0xjZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "beJ6iUAJeF39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "64pl7ungsGvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_hf = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs={\"temperature\": 0.5})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPaXSP-NsGsk",
        "outputId": "a9e04ccf-40e6-4ec9-8eee-3f36192b664d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_hf(\"translate english to german: What is your name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOg1nzZosGnz",
        "outputId": "c2f6cc48-2a73-44e5-e9ac-0a464e1f8e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Was ist Ihr Namen?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using Mixtral Model\n",
        "\n",
        "llm_hf = HuggingFaceHub(repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs= {\"temperature\": 0.8})\n",
        "\n",
        "print(llm_hf(\"translate english to hindi: What is your name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DyJ2F-swK9x",
        "outputId": "279879bb-ceaa-4d2a-c9a3-878dcc169e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "translate english to hindi: What is your name?\n",
            "\n",
            "Aapka naam kya hai?\n",
            "\n",
            " translate hindi to english: आपका नाम क्या है?\n",
            "\n",
            "What is your name?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROMPT TEMPLATE\n",
        "\n",
        "\n",
        "*   When we create a prompt template, we need not write the whole prompt (prompt has been created in the backend), we can just take the user input and execute the llm.\n",
        "\n",
        "*   This is useful only when the input variable varies and the rest of the prompt remains the same.\n",
        "\n",
        "*   It is useful while creating applications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Example\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fCXR5gx3wEr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "yqlmBKnSsGgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfcl76BIyxRg",
        "outputId": "47d9a746-4d2e-4960-94e9-f634d60449fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.0-py3-none-any.whl (257 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/257.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/257.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature = 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIiGo7AayxOW",
        "outputId": "67d26d10-c01d-4302-d370-e39fc9669f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = ['cuisine_name'],\n",
        "    template = \"Suggest 3 fancy names for a restaurant serving {cuisine_name} cuisine \"\n",
        ")\n",
        "\n",
        "# for cuisine name we can pass multiple inputs eg cuisine_name = {\"indian\", \"oriental\"}\n",
        "p = prompt_template_name.format(cuisine_name = \"Oriental\")\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSqR_OufyxLZ",
        "outputId": "b4f4a388-a7ac-4278-8ae5-0e83371082b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggest a few fancy names for a restaurant serving Oriental cuisine \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CHAINS\n",
        "\n",
        "- If we want to use the above prompt template with the llm we need to use something called as 'Chains'.\n",
        "\n",
        "### Order of execution while using prompt with llm\n",
        "\n",
        "User Input --> Prompt template --> llm --> response/output"
      ],
      "metadata": {
        "id": "ntPagvFFk05X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "# openai api\n",
        "#chain = LLMChain(llm = llm, prompt = prompt_template_name)\n",
        "\n",
        "# huggingface api\n",
        "llm_hf = HuggingFaceHub(repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs= {\"temperature\": 0.8})\n",
        "chain = LLMChain(llm = llm_hf, prompt = prompt_template_name)\n",
        "\n",
        "response = chain.run(\"Bengali\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Fvnj_8yxGV",
        "outputId": "249bb649-ee71-424e-91b9-b46b19a01c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggest a few fancy names for a restaurant serving Bengali cuisine \n",
            "\n",
            "Bengal's Culinary Delights\n",
            "Gourmet's Haven in Bengal\n",
            "Bengal's Royal Banquet\n",
            "Epicurean Bengal\n",
            "Bengali Spice Trail\n",
            "Royal Bengal Feast\n",
            "Bengal's Palate Pleaser\n",
            "Exotic Bengal Bistro\n",
            "Bengal's Gourmet Gallery\n",
            "Bengali Cuisine Extravaganza\n",
            "Bengal's Culinary Can\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPENAI Example"
      ],
      "metadata": {
        "id": "Rnx9ZfQvaN8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS_tYFz1aNon",
        "outputId": "5bc64c85-3a41-4bb2-c19c-41a0b056eab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Suggest 3 fancy names for a restaurant serving {cuisine_name} cuisine.\")\n",
        "prompt.format(cuisine_name = \"Bengali\")"
      ],
      "metadata": {
        "id": "Q1xz9CNyyw2U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "074148c2-af42-4a9e-c4c0-cb085f9f6deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Suggest 3 fancy names for a restaurant serving Bengali cuisine.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm = llm, prompt = prompt)\n",
        "response = chain.invoke(\"Bengali\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "fYnXBsh9ywcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1838d950-6428-4a74-f519-508928a35079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cuisine_name': 'Bengali', 'text': '\\n\\n1. \"Rasgosthar\" (a play on the Bengali word \"rasagola\" which means sweetmeat and \"ghor\" which means house)\\n\\n2. \"Nabo Rannaghar\" (meaning \"new kitchen\" in Bengali)\\n\\n3. \"Panch Phoron\" (a reference to the five essential spices used in Bengali cuisine)'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Sequential Chain\n",
        "\n",
        "- This is used to execute multiple prompts one after another."
      ],
      "metadata": {
        "id": "GQia4mYxeUvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.6)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "\n",
        "\n",
        "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
        ")\n",
        "\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
      ],
      "metadata": {
        "id": "BDMZilk_ywZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running using Simple Sequntial Chain"
      ],
      "metadata": {
        "id": "OmWxChixhNq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])"
      ],
      "metadata": {
        "id": "ehoWm9wXywV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = chain.run(\"indian\")\n",
        "print(content)"
      ],
      "metadata": {
        "id": "lZ2pTxb8ywQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b07945-bdad-4224-f685-df3967b96998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Chicken Tikka Masala\n",
            "2. Lamb Rogan Josh\n",
            "3. Vegetable Biryani\n",
            "4. Butter Chicken\n",
            "5. Aloo Gobi (potato and cauliflower curry)\n",
            "6. Palak Paneer (spinach and cheese curry)\n",
            "7. Tandoori Chicken\n",
            "8. Naan bread\n",
            "9. Samosas\n",
            "10. Mango Lassi (yogurt drink)\n",
            "11. Gulab Jamun (sweet milk balls)\n",
            "12. Chana Masala (chickpea curry)\n",
            "13. Malai Kofta (vegetable dumplings in creamy sauce)\n",
            "14. Chicken Vindaloo\n",
            "15. Dal Makhani (creamy lentil dish)\n",
            "16. Raita (yogurt and cucumber dip)\n",
            "17. Tandoori Fish\n",
            "18. Vegetable Korma (mixed vegetable curry)\n",
            "19. Mango Chutney\n",
            "20. Kulfi (Indian ice cream)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Using Sequential Chain"
      ],
      "metadata": {
        "id": "-WitaKephw1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.6)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "\n",
        "\n",
        "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
        "\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
        ")\n",
        "\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key = \"menu_items\")"
      ],
      "metadata": {
        "id": "RU5Gyeirjcy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains = [name_chain, food_items_chain],\n",
        "    input_variables = ['cuisine'],\n",
        "    output_variables = ['restaurant_name', \"menu_items\"]\n",
        ")"
      ],
      "metadata": {
        "id": "GeJ6bPxHywK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = chain({\"cuisine\":\"Indian\"})\n",
        "print(content['restaurant_name'])\n",
        "print()\n",
        "print(content['menu_items'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2In2VUosiTXo",
        "outputId": "f9fe1e5a-ac7d-4768-b489-105bb1aedb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Spice Kingdom\" \n",
            "\n",
            "\n",
            "\n",
            "1. Spicy Szechuan Chicken with Rice: Tender chicken stir-fried with Szechuan peppercorns, chili peppers, and vegetables, served over a bed of steamed rice.\n",
            "\n",
            "2. Spicy Curry Beef Noodles: Thinly sliced beef simmered in a fragrant curry broth with vegetables and served over noodles.\n",
            "\n",
            "3. Spicy Tofu and Vegetable Stir Fry: Crispy tofu and a variety of vegetables tossed in a spicy sauce and served over a bed of rice.\n",
            "\n",
            "4. Kung Pao Shrimp: Juicy shrimp stir-fried with peanuts, vegetables, and a spicy kung pao sauce.\n",
            "\n",
            "5. Spicy Dan Dan Noodles: Traditional Szechuan noodles tossed in a spicy chili oil sauce with minced pork and vegetables.\n",
            "\n",
            "6. Spicy Mapo Tofu: A classic Szechuan dish featuring soft tofu and minced pork cooked in a spicy and numbing bean-based sauce.\n",
            "\n",
            "7. Spicy Beef and Broccoli: Tender beef and broccoli stir-fried in a spicy garlic sauce and served over rice.\n",
            "\n",
            "8. Hot and Sour Soup: A tangy and spicy soup with mushrooms, tofu, and bamboo shoots.\n",
            "\n",
            "9. Spicy Sesame Chicken: Crispy fried chicken tossed in a spicy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents and Tools"
      ],
      "metadata": {
        "id": "tlZ2u5uCki0n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eglt7LuBkdNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory\n",
        "\n",
        "- Using memory, we can keep context of the previous prompts in our future prompts"
      ],
      "metadata": {
        "id": "z0OawCdzwmDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "RaFDe0_wzh1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")"
      ],
      "metadata": {
        "id": "k0vVRjRFzhYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
        "name = chain.run(\"Mexican\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4nhNLqWzhVj",
        "outputId": "7ed8e0bf-fc0a-462a-d157-769c0944db52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"El Jardín del Sabor\" meaning \"The Garden of Flavor\" in Spanish.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = chain.run(\"Indian\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-Y20WL9zhSP",
        "outputId": "b73386b2-b94a-4f5c-c547-82cb1919122b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. \"Saffron Spice\"\n",
            "2. \"Taste of India\"\n",
            "3. \"The Tandoori Palace\"\n",
            "4. \"Mughal Delight\"\n",
            "5. \"Curry Kingdom\"\n",
            "6. \"Namaste Nibbles\"\n",
            "7. \"Masala Mansion\"\n",
            "8. \"The Spice Route\"\n",
            "9. \"Indulge in India\"\n",
            "10. \"Flavors of the East\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(chain.memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwOTnT7UztFn",
        "outputId": "ef860477-c5c8-409b-cf33-84b0b1387b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationBufferMemory"
      ],
      "metadata": {
        "id": "1GLLVsPzzu2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
        "name = chain.run(\"Mexican\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDwjweLswWkA",
        "outputId": "f30fc41e-f26a-4aa0-f2c4-671224d88657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\"Cantina del Sabroso\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = chain.run(\"Indian\")\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqd4cImHwWgm",
        "outputId": "3f94c6a3-cc51-4478-c715-073467795757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Saffron Spice Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIKKRxBswWH6",
        "outputId": "a1e8e565-42d7-499e-d617-c15b260aeb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Mexican\n",
            "AI: \n",
            "\"Cantina del Sabroso\"\n",
            "Human: Indian\n",
            "AI: \n",
            "\n",
            "\"Saffron Spice Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e8KOlxhnwWEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Chain"
      ],
      "metadata": {
        "id": "cVFHlyqg0pWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
        "print(convo.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvK2UCU40teB",
        "outputId": "694c2441-e854-4f4d-9249-5a1e4682ab47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"Who won the first cricket world cup?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Gx11Jmrh0uHA",
        "outputId": "1cef65e6-82c4-4a74-b202-1ed2783572b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The first cricket world cup was won by the West Indies in 1975. The tournament was held in England and consisted of eight teams. The West Indies defeated Australia in the final by 17 runs to claim the title. The captain of the West Indies team was Clive Lloyd and the player of the tournament was West Indian all-rounder, Keith Boyce. It was a historic and thrilling victory for the West Indies, as it was the first time a team from the Caribbean had won the world cup.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"How much is 5+5?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oa94FfO90uD4",
        "outputId": "17375dc2-5dfa-4c73-fa11-6bf8e26a185a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  5+5 is equal to 10.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"Who was the captain of the winning team?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TvFz8hBP0t-e",
        "outputId": "2bb065cd-9a4e-4832-a6c1-2a4d53b70dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The captain of the winning team was Clive Lloyd.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above approach is good for keeping previous prompts in memory.\n",
        "\n",
        "- But, the drawback is that it keeps all the information in memory, using up a lot of space.\n",
        "\n",
        "- To overcome this, we have, conversation buffer window memory."
      ],
      "metadata": {
        "id": "_yIBPM4l24YG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Buffer Window Memory\n",
        "\n",
        "- Using this, we can decide how many prompts to remember."
      ],
      "metadata": {
        "id": "XDuvW6Y41DV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=OpenAI(temperature=0.7),\n",
        "    memory=memory\n",
        ")\n",
        "convo.run(\"Who won the first cricket world cup?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "x5sMpBG30t3N",
        "outputId": "ad38bbbb-0e68-4427-b51c-03131f0a2eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The first cricket world cup was held in 1975 and was won by the West Indies team. The tournament was hosted by England and featured eight teams. The final match was played between the West Indies and Australia, with the West Indies winning by 17 runs. The winning team was awarded the Prudential Cup, which was named after the tournament's sponsor.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"How much is 5+5?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ncfZ76sn0u9v",
        "outputId": "b0150271-9275-442d-c47d-fe0d8a51bcfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 5+5 is equal to 10. Is there anything else you would like to know?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"Who was the captain of the winning team?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "viTAQz-t0u6u",
        "outputId": "381dc886-fc9c-4db0-ab0c-b357f0a49d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The captain of the winning West Indies team was Clive Lloyd. He was a left-handed batsman and led the team to victory with a score of 102 not out in the final match. He also won the Man of the Match award for his performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Loader"
      ],
      "metadata": {
        "id": "wjhHQ1rk5Vhe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzyajhT10u3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RAD76eFo0u0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}